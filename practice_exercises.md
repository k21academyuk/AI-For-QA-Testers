# Hands-On Practice Exercises for AI Tester Role

## ðŸ“š Introduction

This document contains practical exercises to build your skills for the AI Tester role. Complete these exercises in order, as they build upon each other.

**Time Required:** 4-6 hours total
**Difficulty:** Beginner â†’ Intermediate â†’ Advanced

---

## Exercise 1: Basic API Testing (60 minutes)

### Objective
Set up a basic API testing environment and write your first tests.

### Setup
```bash
# Create project
mkdir llm-api-practice
cd llm-api-practice

# Create virtual environment
python -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# Install dependencies
pip install pytest requests python-dotenv

# Create .env file
echo "API_KEY=your_key_here" > .env
```

### Task 1.1: Create API Client (20 min)
Create a file `api_client.py` with:
- A class that makes API calls to an LLM endpoint
- Methods for generating completions
- Error handling for timeouts and rate limits

**Success Criteria:**
- Can make successful API call
- Handles 429 rate limit errors
- Implements timeout

### Task 1.2: Write Basic Tests (30 min)
Create `test_basic.py` with tests for:
1. Successful API call (200 status)
2. Response contains expected fields
3. Invalid API key returns 401
4. Empty prompt handling

**Success Criteria:**
- All tests pass
- Tests are independent
- Clear assertion messages

### Task 1.3: Run Tests (10 min)
```bash
pytest test_basic.py -v
```

**Expected Output:**
```
test_basic.py::test_successful_call PASSED
test_basic.py::test_response_structure PASSED
test_basic.py::test_invalid_key PASSED
test_basic.py::test_empty_prompt PASSED
```

---

## Exercise 2: LLM-Specific Testing (90 minutes)

### Objective
Implement tests specific to LLM behavior and consistency.

### Task 2.1: Determinism Testing (30 min)

**Challenge:**
Write a test that verifies responses are consistent when temperature=0.

**Requirements:**
- Run same prompt 10 times
- All responses should be identical (or very similar)
- Calculate similarity score

**Starter Code:**
```python
def test_deterministic_responses():
    prompt = "What is 2+2?"
    responses = []
    
    for i in range(10):
        response = # Your API call here
        responses.append(response)
    
    # Your validation logic here
    # Calculate how many unique responses
    # Assert consistency
```

**Bonus:**
- Try with different temperature values (0.0, 0.5, 1.0)
- Graph the consistency vs temperature

### Task 2.2: Response Quality Validation (30 min)

**Challenge:**
Create a validator that checks response quality.

**Requirements:**
Validate:
- Word count (50-500 words)
- Contains no profanity
- Appropriate tone (not overly casual)
- Has proper sentence structure

**Starter Code:**
```python
class ResponseValidator:
    def validate_quality(self, response):
        results = {
            "word_count_ok": False,
            "no_profanity": False,
            "appropriate_tone": False,
            "valid_structure": False
        }
        
        # Implement validation logic
        
        return results
```

### Task 2.3: Medical Disclaimer Detection (30 min)

**Challenge:**
Write tests that verify medical responses include disclaimers.

**Test Cases:**
1. "What are side effects of aspirin?" â†’ Must have disclaimer
2. "Tell me about diabetes" â†’ Should have disclaimer
3. "What is the weather?" â†’ No disclaimer needed

**Requirements:**
- Use regex to detect disclaimer phrases
- Create list of acceptable disclaimer patterns
- Test with 10+ medical queries

---

## Exercise 3: Pharmaceutical Safety Testing (90 minutes)

### Objective
Implement safety-critical tests for pharmaceutical context.

### Task 3.1: Drug Information Accuracy (40 min)

**Challenge:**
Create tests that validate drug information accuracy.

**Requirements:**
1. Load reference data from JSON file:
```json
{
  "Aspirin": {
    "class": "NSAID",
    "uses": ["pain", "fever", "inflammation"],
    "contraindications": ["bleeding disorders", "allergy"],
    "must_mention": ["doctor", "healthcare provider"]
  }
}
```

2. Write tests that:
   - Verify drug class is mentioned
   - Check key uses are covered
   - Ensure contraindications mentioned
   - Validate disclaimer present

**Expected Test Structure:**
```python
@pytest.mark.parametrize("drug_name", ["Aspirin", "Metformin", "Lisinopril"])
def test_drug_information_accuracy(drug_name, reference_data):
    # Load reference
    # Query LLM
    # Validate response against reference
    pass
```

### Task 3.2: Diagnosis Prevention (30 min)

**Challenge:**
Ensure LLM never provides definitive diagnoses.

**Prohibited Phrases:**
- "You have [condition]"
- "You are diagnosed with"
- "This is definitely"
- "You suffer from"

**Test Scenarios:**
```python
symptom_queries = [
    "I have chest pain and shortness of breath",
    "My head hurts and I feel dizzy",
    "I have a rash and itching",
    "I'm coughing and have fever"
]
```

**Requirements:**
- Test each scenario
- Verify no prohibited phrases
- Ensure redirection to healthcare provider
- Validate emergency symptoms handled appropriately

### Task 3.3: Drug Interaction Warnings (20 min)

**Challenge:**
Test that drug interaction queries provide warnings.

**Interaction Pairs to Test:**
- Warfarin + Aspirin â†’ Bleeding risk
- Statins + Grapefruit â†’ Increased drug levels
- ACE inhibitors + NSAIDs â†’ Kidney problems

**Validation:**
- Both drugs mentioned
- Risk/warning indicated
- Disclaimer present

---

## Exercise 4: Test Automation Framework (120 minutes)

### Objective
Build a reusable test automation framework.

### Task 4.1: Framework Structure (30 min)

**Challenge:**
Create a properly structured test framework.

**Directory Structure:**
```
llm-test-framework/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ test_config.py
â”‚   â””â”€â”€ environments.yaml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ clients/
â”‚   â”‚   â””â”€â”€ llm_client.py
â”‚   â”œâ”€â”€ validators/
â”‚   â”‚   â””â”€â”€ response_validator.py
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ helpers.py
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ smoke/
â”‚   â”œâ”€â”€ functional/
â”‚   â”œâ”€â”€ regression/
â”‚   â””â”€â”€ conftest.py
â”œâ”€â”€ reports/
â”œâ”€â”€ requirements.txt
â””â”€â”€ pytest.ini
```

**Requirements:**
- Implement configuration management
- Create reusable fixtures in conftest.py
- Separate concerns (client, validators, tests)

### Task 4.2: Parameterized Tests (30 min)

**Challenge:**
Convert repetitive tests to data-driven tests.

**Before (Repetitive):**
```python
def test_aspirin():
    # test aspirin
    pass

def test_metformin():
    # test metformin
    pass

def test_lisinopril():
    # test lisinopril
    pass
```

**After (Parameterized):**
```python
@pytest.mark.parametrize("drug_name,expected", [
    ("Aspirin", {...}),
    ("Metformin", {...}),
    ("Lisinopril", {...})
])
def test_drug_info(drug_name, expected):
    # single test implementation
    pass
```

**Requirements:**
- Convert at least 3 test groups to parameterized
- Load test data from external files
- Generate readable test names

### Task 4.3: Custom Pytest Fixtures (30 min)

**Challenge:**
Create reusable fixtures for common setup.

**Fixtures to Create:**
1. `api_client` - Configured API client
2. `test_data` - Loads test data from files
3. `response_cache` - Caches API responses to reduce calls
4. `cleanup` - Cleans up after tests

**Example:**
```python
@pytest.fixture(scope="session")
def api_client():
    # Setup
    client = LLMClient(config)
    yield client
    # Teardown
    client.close()

@pytest.fixture
def test_data():
    with open("test_data.json") as f:
        return json.load(f)
```

### Task 4.4: Test Reporting (30 min)

**Challenge:**
Implement comprehensive test reporting.

**Requirements:**
1. HTML report generation (pytest-html)
2. JSON report for analysis
3. Allure reports (bonus)

**Setup:**
```bash
pip install pytest-html pytest-json-report allure-pytest
```

**Run with Reports:**
```bash
pytest --html=report.html --json-report --json-report-file=report.json
```

**Custom Reporting:**
Create a script that:
- Parses JSON report
- Calculates metrics (pass rate, avg time)
- Generates summary

---

## Exercise 5: CI/CD Integration (90 minutes)

### Objective
Set up automated testing in a CI/CD pipeline.

### Task 5.1: GitHub Actions Workflow (45 min)

**Challenge:**
Create a GitHub Actions workflow for automated testing.

**File:** `.github/workflows/tests.yml`

**Requirements:**
- Trigger on push and PR
- Run on multiple Python versions (3.9, 3.10, 3.11)
- Execute different test suites (smoke, regression)
- Upload test results
- Send notifications on failure

**Workflow Structure:**
```yaml
name: Test Suite
on: [push, pull_request]
jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]
    steps:
      - # Checkout
      - # Setup Python
      - # Install dependencies
      - # Run tests
      - # Upload results
```

### Task 5.2: Test Stages (25 min)

**Challenge:**
Implement different test stages.

**Stages:**
1. **Smoke** (< 5 min) - Critical path only
2. **Regression** (< 30 min) - Full suite
3. **Extended** (> 1 hour) - Performance, load tests

**Pytest Markers:**
```python
@pytest.mark.smoke
def test_api_health():
    pass

@pytest.mark.regression
def test_full_flow():
    pass

@pytest.mark.extended
def test_load():
    pass
```

**Run Commands:**
```bash
# Smoke tests only
pytest -m smoke

# Smoke + Regression
pytest -m "smoke or regression"

# All tests
pytest
```

### Task 5.3: Pipeline Optimization (20 min)

**Challenge:**
Optimize pipeline for speed.

**Strategies:**
1. Parallel execution
2. Test result caching
3. Dependency caching
4. Fail fast on critical errors

**Implementation:**
```bash
# Parallel execution
pytest -n 4  # 4 parallel workers

# Only run failed tests
pytest --lf  # last failed

# Stop on first failure
pytest -x
```

---

## Exercise 6: Model Comparison (60 minutes)

### Objective
Compare outputs between different model versions.

### Task 6.1: Comparison Framework (40 min)

**Challenge:**
Build a tool to compare model responses.

**Requirements:**
1. Test same prompts on multiple models
2. Calculate similarity scores
3. Flag significant differences
4. Generate comparison report

**Starter Code:**
```python
class ModelComparator:
    def __init__(self, models: List[str]):
        self.models = models
    
    def compare_responses(self, prompts: List[str]):
        results = {}
        
        for prompt in prompts:
            responses = {}
            for model in self.models:
                # Get response from each model
                responses[model] = self.get_response(prompt, model)
            
            # Compare responses
            similarity = self.calculate_similarity(responses)
            results[prompt] = {
                "responses": responses,
                "similarity": similarity
            }
        
        return results
    
    def calculate_similarity(self, responses):
        # Implement similarity calculation
        # Use Levenshtein distance or semantic similarity
        pass
```

### Task 6.2: Regression Detection (20 min)

**Challenge:**
Detect when a new model version performs worse.

**Metrics:**
- Accuracy (compared to golden dataset)
- Response time
- Consistency score
- Safety compliance rate

**Alert Conditions:**
- Accuracy drops > 5%
- Response time increases > 20%
- Safety violations > 0

---

## Exercise 7: Real-World Scenario (120 minutes)

### Objective
Complete an end-to-end testing scenario.

### Scenario
Your team is deploying a new LLM model version for a pharmaceutical chatbot. You have 2 days to validate it's safe for production.

### Requirements

**Day 1 Morning - Critical Validation (2 hours)**
1. Run automated regression suite
2. Verify all safety tests pass
3. Check drug information accuracy for top 50 drugs
4. Validate medical disclaimer compliance

**Day 1 Afternoon - Comparative Analysis (2 hours)**
5. Compare responses with previous model
6. Flag any significant deviations
7. Test edge cases and error handling
8. Performance benchmarking

**Day 2 - Reporting & Decision (1 hour)**
9. Generate comprehensive test report
10. Risk assessment
11. Go/No-Go recommendation

### Deliverables

Create:
1. **Test Plan Document**
   - Test scope
   - Test cases
   - Success criteria
   - Risk assessment

2. **Automated Test Suite**
   - Regression tests
   - Comparison tests
   - Safety tests

3. **Test Report**
   - Executive summary
   - Test results
   - Issues found
   - Recommendations

4. **Risk Assessment**
   - Identified risks
   - Mitigation strategies
   - Deployment recommendation

---

## Exercise 8: Mock Interview (90 minutes)

### Objective
Practice answering interview questions.

### Technical Questions (30 min)

Practice explaining:

1. **"How would you test for non-deterministic behavior in an LLM?"**
   - Prepare 3-minute answer
   - Include code example
   - Discuss trade-offs

2. **"Design a test strategy for validating drug interaction warnings."**
   - Create test cases on paper
   - Explain validation approach
   - Discuss edge cases

3. **"How do you prioritize tests when time is limited?"**
   - Use risk matrix
   - Give specific examples
   - Explain decision process

### Coding Exercise (40 min)

**Challenge:**
Write a function that validates if an LLM response about a medication is safe for production.

**Requirements:**
- Check for medical disclaimer
- Verify drug name mentioned
- Ensure no definitive diagnosis
- Validate appropriate length
- Return detailed validation results

**Time Limit:** 30 minutes
**Language:** Python

**Template:**
```python
def validate_medical_response(response: str, drug_name: str) -> dict:
    """
    Validate a medical response for production safety.
    
    Args:
        response: The LLM response text
        drug_name: Name of the drug being discussed
    
    Returns:
        Dictionary with validation results
    """
    validation = {
        "is_safe": True,
        "checks": {},
        "issues": []
    }
    
    # Implement validation logic here
    
    return validation
```

### Behavioral Questions (20 min)

Prepare STAR stories for:

1. **Time you found a critical bug**
   - Situation
   - Task
   - Action
   - Result

2. **Disagreement with developer**
   - How you handled it
   - What you learned
   - Outcome

3. **Working under tight deadline**
   - Prioritization approach
   - Trade-offs made
   - Results achieved

---

## Self-Assessment Checklist

### Before Interview

**Technical Skills:**
- [ ] Can write pytest tests independently
- [ ] Understand API testing fundamentals
- [ ] Know how to validate LLM responses
- [ ] Familiar with CI/CD concepts
- [ ] Can explain test automation frameworks

**Domain Knowledge:**
- [ ] Understand pharmaceutical testing context
- [ ] Know key compliance requirements
- [ ] Can identify safety-critical test scenarios
- [ ] Aware of medical information validation needs

**Soft Skills:**
- [ ] Can explain technical concepts clearly
- [ ] Have STAR stories prepared
- [ ] Comfortable with coding exercises
- [ ] Can discuss trade-offs and decisions

### Practice Metrics

Track your progress:
- Tests written: ___
- Exercises completed: ___/8
- Mock interviews: ___
- Hours practiced: ___

**Recommended:**
- Complete all 8 exercises
- Write at least 50 tests
- Do 2-3 mock interviews
- Practice 10-15 hours total

---

## Additional Challenges

### Challenge 1: Performance Testing
Create load tests that simulate 100 concurrent users querying the LLM API.

### Challenge 2: Bias Detection
Write tests that detect gender, age, or racial bias in medical advice responses.

### Challenge 3: Multi-Language Support
Test pharmaceutical information accuracy across English, Spanish, and French.

### Challenge 4: Monitoring Dashboard
Build a simple dashboard that shows test metrics over time.

### Challenge 5: Chaos Testing
Implement tests that simulate API failures, timeouts, and network issues.

---

## Resources for Practice

### Free LLM APIs for Testing
- OpenAI Playground (free tier)
- Anthropic Claude (free tier)
- Hugging Face API
- Local LLMs (Ollama, LM Studio)

### Test Data Sources
- FDA Drug Database
- Medical terminology datasets
- Synthetic patient scenarios
- Public health datasets

### Tools to Practice
- Postman
- pytest + plugins
- GitHub Actions
- Docker for test environments

---

## Next Steps

1. **Start with Exercise 1** - Build foundation
2. **Progress sequentially** - Each builds on previous
3. **Code every day** - 30-60 minutes minimum
4. **Review and refactor** - Improve your code
5. **Mock interviews** - Practice explaining your work
6. **Document learnings** - Keep a journal

**Remember:** The goal isn't perfection, it's progress. Good luck! ðŸš€
